
# ğŸš€ Big Data Log Processing Pipeline


![Project Banner](ReadmeFigures/Mini_BigDataPipeline_simple_compose.png) 


> A mini Big Data pipeline that demonstrates real-time log generation, ingestion using Apache Flume, storage in HDFS, and data analysis using Apache Spark.

---

## ğŸ“Œ Project Overview

This project was developed as the final project for the **Big Data Analysis Course at NTI**. It simulates a real-time log processing system using popular big data tools.

The pipeline consists of:

1. **Python script** generating synthetic log data.
2. **Apache Flume** capturing and ingesting logs.
3. **HDFS** storing logs in a structured format.
4. **Apache Spark** analyzing and extracting insights from the logs.

---

## ğŸ”„ System Architecture

![Architecture Diagram](ReadmeFigures/Mini_BigDataPipeline_simple_compose2.png)
> *Figure: Overview of the log processing pipeline showing integration of Python, Flume, HDFS, and Spark.*

---

## ğŸ“‚ Project Structure

```bash
logToFlumeHDFS_Spark/
â”‚
â”œâ”€â”€ log_generator/
â”‚   â””â”€â”€ logGen.py       # Python script to generate log data
â”‚
â”œâ”€â”€ flume_config/
â”‚   â””â”€â”€ exercise5.conf         # Apache Flume configuration file
â”‚
â”œâ”€â”€ spark_analysis/
â”‚   â”œâ”€â”€ analysis.ipynb         # Jupyter Notebook for log analysis
â”‚   â””â”€â”€ analysis.py            # PySpark script (optional CLI version)
â”‚
â”œâ”€â”€ ReadmeFigures/
â”‚   â”œâ”€â”€ banner.png             # Project banner image
â”‚   â”œâ”€â”€ Mini_BigDataPipeline_simple_compose.png # Project pipeline diagram
â”‚   â””â”€â”€ demo.gif               
â”‚
â””â”€â”€ README.md

```


## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Generate Logs

```bash
python3 log_generator/generate_logs.py
```

### 2ï¸âƒ£ Start Apache Flume Agent

```bash
$FLUME_HOME/bin/flume-ng agent \
  -c $FLUME_HOME/conf/ \
  -f flume_config/exercise5.conf \
  --name a1 \
  -Dflume.root.logger=INFO,console
```

### 3ï¸âƒ£ View Data in HDFS

```bash
hdfs dfs -ls /tmp/logGenED/
```

### 4ï¸âƒ£ Analyze Logs with Spark

Using `pyspark` shell:

```python
df = spark.read.text("/tmp/logGenED/...")
# Add transformations here...
```

Or use the Jupyter Notebook in `spark_analysis/`.

---

## ğŸ“Š Sample Output

![Log Analysis Result](assets/sample-output.png)

> *Figure: Sample result showing log counts by level using PySpark.*

---

## ğŸ¥ Demo

![Pipeline Demo](ReadmeFigures/demo.gif)

> *Figure: Animated demo showing real-time log generation and analysis.*

---

## ğŸ› ï¸ Technologies Used

* ğŸ Python 3
* ğŸ”¥ Apache Flume
* ğŸ˜ HDFS (Hadoop Distributed File System)
* âš¡ Apache Spark (PySpark)
* ğŸ“ Jupyter Notebook

---

## ğŸ“š Future Work

* Add Kafka integration instead of Flume.
* Store in Parquet/Avro format.
* Create dashboards using Apache Superset or Power BI.
* Containerize the system using Docker.

---

## ğŸ¤ Acknowledgements

Developed during the NTI Big Data Analysis Course
Special thanks to the instructors and course team.

---

## ğŸ“¬ Contact

**Abdelrahman Anwar**
For questions, feedback, or collaboration, feel free to reach out:

- **Emial**: [abd.ahm.anwar@gamil.com](mailto:abd.ahm.anwar@gamil.com)
- **GitHub**: [Abdo-Anwar](https://github.com/Abdo-Anwar)
- **LinkedIn**: [abdelrhman-anwar](https://www.linkedin.com/in/abdelrhman-anwar)


