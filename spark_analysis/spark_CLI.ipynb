{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5828442e",
   "metadata": {},
   "source": [
    "# ðŸ” Log Analysis with PySpark CLI\n",
    "This notebook analyzes log data stored in HDFS and extracts meaningful insights using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a478dfe",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Load Data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read logs from HDFS using the correct full path\n",
    "logs  = spark.sparkContext.textFile(\"hdfs://localhost:9000/tmp/logGenED/25-07-19/*)\n",
    "\n",
    "# Display the first 10 lines of the logs\n",
    "logs.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab6abc9",
   "metadata": {},
   "source": [
    "## Analysis Opreation ðŸ”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LogAnalysis\").getOrCreate()\n",
    "rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/tmp/logGenED/25-07-19/*\"\n",
    "\n",
    "# Set log level to avoid unnecessary output\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "# Define the parser function\n",
    "def parse_log(line):\n",
    "    parts = line.split(\" - \")\n",
    "    timestamp = parts[0].strip()\n",
    "    level = parts[1].strip()\n",
    "    service = parts[2].strip()\n",
    "    response_str = parts[3].split()[-1]\n",
    "    try:\n",
    "        response_time = int(response_str.replace(\"ms\", \"\"))\n",
    "    except:\n",
    "        response_time = 0\n",
    "    return (timestamp, level, service, response_time)\n",
    "\n",
    "# Create parsed RDD and cache it for multiple uses\n",
    "parsed_rdd = rdd.map(parse_log).filter(lambda x: x[3] > 0).cache()\n",
    "\n",
    "# Run analyses\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Total log entries: {rdd.count()}\")\n",
    "\n",
    "print(\"\\nLog level distribution:\")\n",
    "level_counts = parsed_rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "for level, count in sorted(level_counts):\n",
    "    print(f\"{level}: {count}\")\n",
    "\n",
    "print(\"\\nService distribution:\")\n",
    "service_counts = parsed_rdd.map(lambda x: (x[2], 1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "for service, count in sorted(service_counts):\n",
    "    print(f\"{service}: {count}\")\n",
    "\n",
    "print(\"\\nAverage response time by service:\")\n",
    "service_times = parsed_rdd.map(lambda x: (x[2], (x[3], 1)))\n",
    "service_avg = service_times.reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1])).mapValues(lambda x: x[0]/x[1]).collect()\n",
    "for service, avg in sorted(service_avg):\n",
    "    print(f\"{service}: {avg:.2f}ms\")\n",
    "\n",
    "print(\"\\nTop 5 slowest responses:\")\n",
    "slowest = parsed_rdd.map(lambda x: (x[3], f\"{x[0]} | {x[1]} | {x[2]}\")).top(5)\n",
    "for time, entry in slowest:\n",
    "    print(f\"{time}ms: {entry}\")\n",
    "\n",
    "# Unpersist cached data\n",
    "parsed_rdd.unpersist()\n",
    "print(\"=\"*50)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ecd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
